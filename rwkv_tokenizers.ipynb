{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cahya-wirawan/rwkv-tokenizer/blob/main/rwkv_tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xg3vkPt70OV"
      },
      "source": [
        "# Comparison of RWKV Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "S3ZLIOIPoIQu",
        "outputId": "dfa82260-0437-4fd7-db8c-9111823a9eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rwkv in /usr/local/lib/python3.10/dist-packages (0.8.26)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: rwkv_tokenizer in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from rwkv) (0.19.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!apt install cargo\n",
        "!pip install rwkv datasets rwkv_tokenizer tiktoken\n",
        "!pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "du7z3X6hnoJJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import rwkv\n",
        "from rwkv import rwkv_tokenizer\n",
        "from rwkv.rwkv_tokenizer import TRIE_TOKENIZER\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import rwkv_tokenizer\n",
        "import tiktoken\n",
        "import tqdm\n",
        "import plotly.express as px\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9iPfcouobsf",
        "outputId": "f4a06700-fd59-436a-c5e8-05bd0b7a5a65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'url', 'title', 'text'],\n",
            "        num_rows: 205328\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "#ds = load_dataset(\"wikitext\", \"wikitext-103-v1\")\n",
        "#ds = load_dataset(\"cahya/datasets-test\")\n",
        "ds = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "print(ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XWOSSX8yoe3T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17e54f49-6bcb-4f48-ef43-231f52026816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "205328\n"
          ]
        }
      ],
      "source": [
        "MAX_ROW = len(ds[\"train\"])\n",
        "print(MAX_ROW)\n",
        "time_diffs = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvsfTjJP8KiF"
      },
      "source": [
        "## The Original RWKV Tokenizer (BlinkDL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HGXhyUMVoDK1"
      },
      "outputs": [],
      "source": [
        "tokenizer_rwkv = TRIE_TOKENIZER(os.path.dirname(os.path.abspath(rwkv.__file__)) + '/rwkv_vocab_v20230424.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-wscjjbohgZ",
        "collapsed": true,
        "outputId": "f7c23da3-a9eb-4860-a118-de1696782f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205328/205328 [03:18<00:00, 1033.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of chars: 215489882\n",
            "Number of tokens: 53619552\n",
            "Chars per token: 4.02\n",
            "Time difference: 198.73s\n",
            "Chars per second: 1084338.67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "chars_number = 0\n",
        "tokens_number = 0\n",
        "start = datetime.now()\n",
        "for i, row in tqdm.tqdm(enumerate(ds[\"train\"]), total=MAX_ROW):\n",
        "    try:\n",
        "      inputs = tokenizer_rwkv.encode(row[\"text\"][:])\n",
        "    except:\n",
        "      print(f\"{i}: {row['text']}\")\n",
        "      break\n",
        "    chars_number += len(row[\"text\"])\n",
        "    tokens_number += len(inputs)\n",
        "    if i>=MAX_ROW:\n",
        "        break\n",
        "end = datetime.now()\n",
        "time_diff = (end-start).total_seconds()\n",
        "print()\n",
        "print(f\"Number of chars: {chars_number}\")\n",
        "print(f\"Number of tokens: {tokens_number}\")\n",
        "print(f\"Chars per token: {chars_number/tokens_number:.2f}\")\n",
        "print(f\"Time difference: {time_diff:.2f}s\")\n",
        "print(f\"Chars per second: {chars_number/time_diff:.2f}\")\n",
        "time_diffs[\"The Original RWKV Tokenizer (BlinkDL)\"] = time_diff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pAqq-Od8Qt8"
      },
      "source": [
        "## The Huggingface RWKV Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Slx1bkbCommr"
      },
      "outputs": [],
      "source": [
        "tokenizer_hf_rwkv = AutoTokenizer.from_pretrained(\"RWKV/rwkv-6-world-3b\", trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BE0JQWlkooyH",
        "outputId": "6258bf0d-31d5-472a-90f0-0937672621db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205328/205328 [11:06<00:00, 308.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of chars: 215489882\n",
            "Number of tokens: 53767631\n",
            "Chars per token: 4.01\n",
            "Time difference: 666.10s\n",
            "Chars per second: 323510.76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "chars_number = 0\n",
        "tokens_number = 0\n",
        "start = datetime.now()\n",
        "for i, row in tqdm.tqdm(enumerate(ds[\"train\"]), total=MAX_ROW):\n",
        "    try:\n",
        "      inputs = tokenizer_hf_rwkv.encode(row[\"text\"][:])\n",
        "    except:\n",
        "      print(f\"{i}: {row['text']}\")\n",
        "      break\n",
        "    chars_number += len(row[\"text\"])\n",
        "    tokens_number += len(inputs)\n",
        "    if i>=MAX_ROW:\n",
        "        break\n",
        "end = datetime.now()\n",
        "time_diff = (end-start).total_seconds()\n",
        "print()\n",
        "print(f\"Number of chars: {chars_number}\")\n",
        "print(f\"Number of tokens: {tokens_number}\")\n",
        "print(f\"Chars per token: {chars_number/tokens_number:.2f}\")\n",
        "print(f\"Time difference: {time_diff:.2f}s\")\n",
        "print(f\"Chars per second: {chars_number/time_diff:.2f}\")\n",
        "time_diffs[\"The Huggingface RWKV Tokenizer\"] = time_diff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6TpepXV8Xwr"
      },
      "source": [
        "## The Huggingface LLama Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wzywva-QoshI"
      },
      "outputs": [],
      "source": [
        "tokenizer_hf_llama = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-7B-AWQ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iMBDEsM7oubI",
        "outputId": "0efea233-d139-4d94-a806-d93ed159b372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205328/205328 [04:32<00:00, 753.76it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of chars: 215489882\n",
            "Number of tokens: 64676875\n",
            "Chars per token: 3.33\n",
            "Time difference: 272.42s\n",
            "Chars per second: 791012.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "chars_number = 0\n",
        "tokens_number = 0\n",
        "start = datetime.now()\n",
        "for i, row in tqdm.tqdm(enumerate(ds[\"train\"]), total=MAX_ROW):\n",
        "    try:\n",
        "      inputs = tokenizer_hf_llama.encode(row[\"text\"][:])\n",
        "    except:\n",
        "      print(f\"{i}: {row['text']}\")\n",
        "      break\n",
        "    chars_number += len(row[\"text\"])\n",
        "    tokens_number += len(inputs)\n",
        "    if i>=MAX_ROW:\n",
        "        break\n",
        "end = datetime.now()\n",
        "time_diff = (end-start).total_seconds()\n",
        "print()\n",
        "print(f\"Number of chars: {chars_number}\")\n",
        "print(f\"Number of tokens: {tokens_number}\")\n",
        "print(f\"Chars per token: {chars_number/tokens_number:.2f}\")\n",
        "print(f\"Time difference: {time_diff:.2f}s\")\n",
        "print(f\"Chars per second: {chars_number/time_diff:.2f}\")\n",
        "time_diffs[\"The Huggingface LLama Tokenizer\"] = time_diff"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The OpenAI Tiktoken"
      ],
      "metadata": {
        "id": "zDHuCtmnX_X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_tiktoken = tiktoken.get_encoding(\"o200k_base\")"
      ],
      "metadata": {
        "id": "DsEDiajKYJy-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars_number = 0\n",
        "tokens_number = 0\n",
        "start = datetime.now()\n",
        "for i, row in tqdm.tqdm(enumerate(ds[\"train\"]), total=MAX_ROW):\n",
        "    try:\n",
        "      inputs = tokenizer_tiktoken.encode(row[\"text\"][:])\n",
        "    except:\n",
        "      print(f\"{i}: {row['text']}\")\n",
        "      break\n",
        "    chars_number += len(row[\"text\"])\n",
        "    tokens_number += len(inputs)\n",
        "    if i>=MAX_ROW:\n",
        "        break\n",
        "end = datetime.now()\n",
        "time_diff = (end-start).total_seconds()\n",
        "print()\n",
        "print(f\"Number of chars: {chars_number}\")\n",
        "print(f\"Number of tokens: {tokens_number}\")\n",
        "print(f\"Chars per token: {chars_number/tokens_number:.2f}\")\n",
        "print(f\"Time difference: {time_diff:.2f}s\")\n",
        "print(f\"Chars per second: {chars_number/time_diff:.2f}\")\n",
        "time_diffs[\"The OpenAI Tiktoken\"] = time_diff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLCien1mZD9i",
        "outputId": "97ccc465-e77f-4f8b-a07f-3c596341e370"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205328/205328 [01:53<00:00, 1805.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of chars: 215489882\n",
            "Number of tokens: 51926917\n",
            "Chars per token: 4.15\n",
            "Time difference: 113.73s\n",
            "Chars per second: 1894773.38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ag_OXgards4"
      },
      "source": [
        "## The new RWKV Tokenizer in Rust (Cahya)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CXzdXYFrY1S",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Just to get the vocab file rwkv_vocab_v20230424.txt\n",
        "!git clone https://github.com/cahya-wirawan/rwkv-tokenizer.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GGtm63qcyRV_"
      },
      "outputs": [],
      "source": [
        "tokenizer_rust = rwkv_tokenizer.Tokenizer(\"rwkv-tokenizer/src/rwkv_vocab_v20230424.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLudQCWZyMtI",
        "outputId": "bbd36303-92e6-40ba-f1ed-7c7330560e88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205328/205328 [00:33<00:00, 6067.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of chars: 215489882\n",
            "Number of tokens: 53619552\n",
            "Chars per token: 4.02\n",
            "Time difference: 33.85s\n",
            "Chars per second: 6366744.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "chars_number = 0\n",
        "tokens_number = 0\n",
        "start = datetime.now()\n",
        "for i, row in tqdm.tqdm(enumerate(ds[\"train\"]), total=MAX_ROW):\n",
        "    try:\n",
        "      inputs = tokenizer_rust.encode(row[\"text\"][:])\n",
        "    except:\n",
        "      print(f\"{i}: {row['text']}\")\n",
        "      break\n",
        "    chars_number += len(row[\"text\"])\n",
        "    tokens_number += len(inputs)\n",
        "    if i>=MAX_ROW:\n",
        "        break\n",
        "end = datetime.now()\n",
        "time_diff = (end-start).total_seconds()\n",
        "print()\n",
        "print(f\"Number of chars: {chars_number}\")\n",
        "print(f\"Number of tokens: {tokens_number}\")\n",
        "print(f\"Chars per token: {chars_number/tokens_number:.2f}\")\n",
        "print(f\"Time difference: {time_diff:.2f}s\")\n",
        "print(f\"Chars per second: {chars_number/time_diff:.2f}\")\n",
        "time_diffs[\"The new RWKV Tokenizer in Rust\"] = time_diff"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The number of tokens encoded by the new rwkv tokenizer in Rust is exactly the same as the number of tokens encoded by the original tokenizer.**"
      ],
      "metadata": {
        "id": "xa90bcngA2gV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0R3E23AJSrE"
      },
      "source": [
        "## Tokens Comparison\n",
        "We compare the encoded tokens between the new Rust rwkv_tokenizer and the original BlinkDL rwkv tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Today is a beautiful day. 今天是美好的一天。\"\n",
        "encoded = tokenizer_rust.encode(text)\n",
        "decoded = tokenizer_rust.decode(encoded)\n",
        "print(f\"encoded: {encoded}\")\n",
        "print(f\"text:\\t\\t{text}\")\n",
        "print(f\"decoded:\\t{decoded}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYlM-JDc9_wN",
        "outputId": "903ef268-0120-47b6-e9b9-bbc6f89c870c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded: [33520, 4600, 332, 59219, 21509, 47, 33, 10381, 11639, 13091, 15597, 11685, 14734, 10250, 11639, 10080]\n",
            "text:\t\tToday is a beautiful day. 今天是美好的一天。\n",
            "decoded:\tToday is a beautiful day. 今天是美好的一天。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### English dataset"
      ],
      "metadata": {
        "id": "EFM5V1Fc9wtD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwgXuzuADAGu",
        "outputId": "cc07ea03-ac5d-4b52-989a-06eee45fdca9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1491: FutureWarning: The repository for wikipedia contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wikipedia\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "100%|██████████| 10000/10000 [00:38<00:00, 261.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Both encoded tokens are exactly the same :-)\n",
            "CPU times: user 34.9 s, sys: 396 ms, total: 35.2 s\n",
            "Wall time: 39.2 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "ds = load_dataset(\"wikipedia\", \"20220301.simple\")\n",
        "MAX_ROW = 10000 #len(ds[\"train\"])\n",
        "counter = 0\n",
        "for i in tqdm.tqdm(range(0, MAX_ROW), total=MAX_ROW):\n",
        "  tokens_rust = tokenizer_rust.encode(ds[\"train\"][i][\"text\"])\n",
        "  tokens_rwkv = tokenizer_rwkv.encode(ds[\"train\"][i][\"text\"])\n",
        "  for index, (rust, rwkv) in enumerate(zip(tokens_rust, tokens_rwkv)):\n",
        "    if rust != rwkv:\n",
        "      print(f\"{i} - {index}: {tokens_rust[max(0,index-4):index+4]}\")\n",
        "      print(f\"{i} - {index}: {tokens_rwkv[max(0,index-4):index+4]}\")\n",
        "      print(f\"{i} - {index}: {tokenizer_rwkv.decode(tokens_rwkv[max(0,index-4):index+4])}\\n\")\n",
        "      counter += 1\n",
        "if counter > 0:\n",
        "  print(f\"\\nFound {counter} difference(s)!\")\n",
        "else:\n",
        "  print(\"\\nBoth encoded tokens are exactly the same :-)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chinese dataset"
      ],
      "metadata": {
        "id": "C3TPdTMF92ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "ds = load_dataset(\"Lifan-Z/Chinese-poetries-txt\")\n",
        "MAX_ROW = 10000 #len(ds[\"train\"])\n",
        "counter = 0\n",
        "for i in tqdm.tqdm(range(0, MAX_ROW), total=MAX_ROW):\n",
        "  tokens_rust = tokenizer_rust.encode(ds[\"train\"][i][\"text\"])\n",
        "  tokens_rwkv = tokenizer_rwkv.encode(ds[\"train\"][i][\"text\"])\n",
        "  for index, (rust, rwkv) in enumerate(zip(tokens_rust, tokens_rwkv)):\n",
        "    if rust != rwkv:\n",
        "      print(f\"{i} - {index}: {tokens_rust[max(0,index-4):index+4]}\")\n",
        "      print(f\"{i} - {index}: {tokens_rwkv[max(0,index-4):index+4]}\")\n",
        "      print(f\"{i} - {index}: {tokenizer_rwkv.decode(tokens_rwkv[max(0,index-4):index+4])}\\n\")\n",
        "      counter += 1\n",
        "if counter > 0:\n",
        "  print(f\"\\nFound {counter} difference(s)!\")\n",
        "else:\n",
        "  print(\"\\nBoth encoded tokens are exactly the same :-)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1JjAFfq7sJH",
        "outputId": "d60c55a6-0c54-452a-8b68-c66078860135"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:02<00:00, 4826.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Both encoded tokens are exactly the same :-)\n",
            "CPU times: user 2.16 s, sys: 78.3 ms, total: 2.24 s\n",
            "Wall time: 3.07 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The tokens comparison shows that the new RWKV tokenizer in Rust encoded exactly the same tokens as the original RWKV tokenizer.**"
      ],
      "metadata": {
        "id": "PehZBxL9AD25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance"
      ],
      "metadata": {
        "id": "tnA7ajE-s6Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({'tokenizer': [name for name in time_diffs], 'time': [time_diffs[name] for name in time_diffs]})\n",
        "fig = px.bar(df, x='tokenizer', y='time')\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "mg4uG72150Mw",
        "outputId": "2ff1002c-e1ec-4833-e1de-9b0008b544df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"d647fb53-0507-406d-995d-183714462012\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d647fb53-0507-406d-995d-183714462012\")) {                    Plotly.newPlot(                        \"d647fb53-0507-406d-995d-183714462012\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"tokenizer=%{x}\\u003cbr\\u003etime=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"The Original RWKV Tokenizer (BlinkDL)\",\"The Huggingface RWKV Tokenizer\",\"The Huggingface LLama Tokenizer\",\"The OpenAI Tiktoken\",\"The new RWKV Tokenizer in Rust\"],\"xaxis\":\"x\",\"y\":[198.729316,666.098042,272.422742,113.728578,33.846166],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"tokenizer\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"time\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d647fb53-0507-406d-995d-183714462012');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unit Test"
      ],
      "metadata": {
        "id": "tKNo1CdnKrRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "src = '''起業家イーロン・マスク氏が創業した宇宙開発企業「スペースX（エックス）」の巨大新型ロケット「スターシップ」が20日朝、初めて打ち上げられたが、爆発した。\n",
        "打ち上げは米テキサス州の東海岸で行われた。無人の試験で、負傷者はいなかった。\n",
        "打ち上げから2～3分後、史上最大のロケットが制御不能になり、まもなく搭載された装置で破壊された。\n",
        "マスク氏は、数カ月後に再挑戦すると表明した。\n",
        "スペースXのエンジニアたちは、それでもこの日のミッションは成功だったとしている。「早期に頻繁に試験する」ことを好む人たちなので、破壊を恐れていない。次のフライトに向け、大量のデータを収集したはずだ。2機目のスターシップは、ほぼ飛行準備が整っている。\n",
        "マスク氏は、「SpaceXチームの皆さん、スターシップのエキサイティングな試験打ち上げ、おめでとう！　数カ月後に行われる次の試験打ち上げに向けて、多くを学んだ」とツイートした。\n",
        "アメリカでのロケット打ち上げを認可する米連邦航空局（NASA）は、事故調査を監督するとした。広報担当者は、飛行中に機体が失われた場合の通常の対応だと述べた。\n",
        "マスク氏は打ち上げ前、期待値を下げようとしていた。発射台の設備を破壊せずに機体を打ち上げるだけでも「成功」だとしていた。\n",
        "その願いはかなった。スターシップは打ち上げ施設からどんどん上昇し、メキシコ湾の上空へと向かっていった。しかし1分もしないうち、すべてが計画通りに進んでいるのではないことが明らかになった。'''"
      ],
      "metadata": {
        "id": "GpHonMRlKq5L"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(src)\n",
        "print(f'\\n{len(src)} chars')\n",
        "tokens = tokenizer_rwkv.encode(src)\n",
        "assert tokenizer_rwkv.decode(tokens) == src\n",
        "print()\n",
        "print(tokens)\n",
        "print(f'\\n{len(tokens)} tokens\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zESxXaHRLPzv",
        "outputId": "504b4ef7-2361-401b-9d1e-3fc6c55036a6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "起業家イーロン・マスク氏が創業した宇宙開発企業「スペースX（エックス）」の巨大新型ロケット「スターシップ」が20日朝、初めて打ち上げられたが、爆発した。\n",
            "打ち上げは米テキサス州の東海岸で行われた。無人の試験で、負傷者はいなかった。\n",
            "打ち上げから2～3分後、史上最大のロケットが制御不能になり、まもなく搭載された装置で破壊された。\n",
            "マスク氏は、数カ月後に再挑戦すると表明した。\n",
            "スペースXのエンジニアたちは、それでもこの日のミッションは成功だったとしている。「早期に頻繁に試験する」ことを好む人たちなので、破壊を恐れていない。次のフライトに向け、大量のデータを収集したはずだ。2機目のスターシップは、ほぼ飛行準備が整っている。\n",
            "マスク氏は、「SpaceXチームの皆さん、スターシップのエキサイティングな試験打ち上げ、おめでとう！　数カ月後に行われる次の試験打ち上げに向けて、多くを学んだ」とツイートした。\n",
            "アメリカでのロケット打ち上げを認可する米連邦航空局（NASA）は、事故調査を監督するとした。広報担当者は、飛行中に機体が失われた場合の通常の対応だと述べた。\n",
            "マスク氏は打ち上げ前、期待値を下げようとしていた。発射台の設備を破壊せずに機体を打ち上げるだけでも「成功」だとしていた。\n",
            "その願いはかなった。スターシップは打ち上げ施設からどんどん上昇し、メキシコ湾の上空へと向かっていった。しかし1分もしないうち、すべてが計画通りに進んでいるのではないことが明らかになった。\n",
            "\n",
            "635 chars\n",
            "\n",
            "[16944, 13436, 11920, 10169, 10242, 10237, 10239, 10241, 10222, 10189, 10179, 13651, 10108, 10827, 13436, 43347, 11887, 11898, 17693, 14728, 10412, 13436, 10088, 10189, 10218, 43528, 89, 19133, 10172, 43484, 10189, 19134, 10089, 10138, 12140, 11638, 13034, 11496, 10237, 10181, 43485, 10088, 58583, 10187, 43486, 10089, 10108, 640, 13053, 13198, 10079, 10782, 43425, 12642, 10127, 10257, 10114, 43435, 43358, 10079, 14288, 14728, 43347, 28329, 12642, 10127, 10257, 10114, 10139, 15260, 10199, 10177, 10185, 10189, 12133, 10138, 13241, 13849, 12061, 10132, 16403, 43448, 10125, 10080, 14215, 10370, 10138, 16572, 18142, 10132, 10079, 16821, 10616, 15643, 10139, 43314, 58526, 28329, 12642, 10127, 10257, 10114, 43328, 51, 19219, 52, 10768, 12341, 10079, 11024, 10257, 13188, 11638, 10138, 10237, 10181, 43485, 10108, 10794, 12354, 10260, 15752, 43410, 10159, 10079, 10148, 10152, 43399, 12883, 17058, 58534, 16451, 15586, 10132, 14894, 11595, 58534, 28329, 10222, 10189, 10179, 13651, 10139, 10079, 13004, 10175, 13190, 12341, 10136, 10703, 12752, 12616, 43352, 10133, 16417, 13078, 43347, 28329, 10189, 10218, 43528, 89, 10138, 10172, 10239, 10188, 10204, 10167, 10125, 10127, 10139, 10079, 43356, 43385, 43340, 13053, 10138, 10223, 10197, 58582, 10139, 12604, 10841, 58542, 58558, 43317, 10080, 10088, 13057, 13199, 10136, 17979, 15414, 10136, 16572, 18142, 43352, 10089, 43339, 10164, 11685, 10150, 10370, 10125, 10127, 43402, 10132, 10079, 14894, 11595, 10164, 12430, 58580, 43397, 10080, 13557, 10138, 10213, 43505, 10201, 10136, 11047, 10113, 10079, 11638, 17387, 10138, 10200, 43530, 10164, 10996, 17848, 43347, 10139, 10122, 10126, 10080, 51, 13521, 14778, 10138, 58583, 10187, 43486, 10139, 10079, 10147, 3022, 189, 18051, 16403, 13982, 10605, 10108, 13006, 64128, 28329, 10222, 10189, 10179, 13651, 10139, 10079, 10088, 33450, 89, 10196, 43534, 10138, 14735, 43344, 10079, 58583, 10187, 43486, 10138, 10172, 10177, 43468, 43487, 43519, 10135, 16572, 18142, 12642, 10127, 10257, 10114, 10079, 10106, 10151, 10132, 10133, 10104, 19126, 10078, 13004, 10175, 13190, 12341, 10136, 16403, 43448, 10160, 13557, 10138, 16572, 18142, 12642, 10127, 10257, 10114, 10136, 11047, 43337, 10079, 11632, 10111, 10164, 11871, 10165, 10126, 10089, 10133, 10198, 10169, 43531, 43347, 28329, 64137, 43383, 10237, 10181, 43485, 12642, 10127, 10257, 10114, 10164, 16587, 11021, 43352, 15260, 17197, 17269, 15878, 15090, 12005, 19133, 23940, 19134, 10139, 10079, 10336, 12985, 16605, 13299, 10164, 14773, 14819, 43352, 43388, 10125, 10080, 12212, 11549, 12701, 12307, 15643, 10139, 10079, 18051, 16403, 10285, 10136, 13521, 10452, 10108, 11645, 43448, 10125, 11550, 11038, 10138, 17189, 12182, 10138, 11961, 12380, 10126, 10133, 17162, 10146, 10125, 28329, 10222, 10189, 10179, 13651, 10139, 12642, 10127, 10257, 10114, 10808, 10079, 13199, 12335, 10567, 10164, 10258, 10114, 43429, 58558, 43310, 10080, 14728, 11965, 11022, 10138, 16552, 10605, 10164, 14894, 11595, 10123, 10122, 10136, 13521, 10452, 10164, 12642, 10127, 10257, 10114, 10160, 43366, 43385, 10088, 12604, 10841, 10089, 10126, 58558, 43310, 28329, 43355, 17988, 10103, 10139, 43327, 43369, 10080, 58583, 10187, 43486, 10139, 12642, 10127, 10257, 10114, 13038, 16552, 43328, 10134, 10165, 10134, 10165, 10257, 13074, 10119, 10079, 10225, 10177, 10187, 10183, 13974, 10138, 10257, 15090, 10145, 10133, 11047, 43326, 43372, 43369, 10080, 43346, 10119, 50, 10768, 10152, 10119, 43397, 10104, 10127, 10079, 10121, 10146, 10131, 10108, 16540, 14604, 17189, 10159, 10136, 17202, 43450, 43317, 43416, 43419, 10103, 58532, 13078, 10158, 10107, 58565, 10125, 10080]\n",
            "\n",
            "519 tokens\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(src)\n",
        "print(f'\\n{len(src)} chars')\n",
        "tokens = tokenizer_rust.encode(src)\n",
        "assert tokenizer_rust.decode(tokens) == src\n",
        "print()\n",
        "print(tokens)\n",
        "print(f'\\n{len(tokens)} tokens\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARU7btTVK0Yj",
        "outputId": "e91404d2-518b-4f29-926f-eaa7270aa63b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "起業家イーロン・マスク氏が創業した宇宙開発企業「スペースX（エックス）」の巨大新型ロケット「スターシップ」が20日朝、初めて打ち上げられたが、爆発した。\n",
            "打ち上げは米テキサス州の東海岸で行われた。無人の試験で、負傷者はいなかった。\n",
            "打ち上げから2～3分後、史上最大のロケットが制御不能になり、まもなく搭載された装置で破壊された。\n",
            "マスク氏は、数カ月後に再挑戦すると表明した。\n",
            "スペースXのエンジニアたちは、それでもこの日のミッションは成功だったとしている。「早期に頻繁に試験する」ことを好む人たちなので、破壊を恐れていない。次のフライトに向け、大量のデータを収集したはずだ。2機目のスターシップは、ほぼ飛行準備が整っている。\n",
            "マスク氏は、「SpaceXチームの皆さん、スターシップのエキサイティングな試験打ち上げ、おめでとう！　数カ月後に行われる次の試験打ち上げに向けて、多くを学んだ」とツイートした。\n",
            "アメリカでのロケット打ち上げを認可する米連邦航空局（NASA）は、事故調査を監督するとした。広報担当者は、飛行中に機体が失われた場合の通常の対応だと述べた。\n",
            "マスク氏は打ち上げ前、期待値を下げようとしていた。発射台の設備を破壊せずに機体を打ち上げるだけでも「成功」だとしていた。\n",
            "その願いはかなった。スターシップは打ち上げ施設からどんどん上昇し、メキシコ湾の上空へと向かっていった。しかし1分もしないうち、すべてが計画通りに進んでいるのではないことが明らかになった。\n",
            "\n",
            "635 chars\n",
            "\n",
            "[16944, 13436, 11920, 10169, 10242, 10237, 10239, 10241, 10222, 10189, 10179, 13651, 10108, 10827, 13436, 43347, 11887, 11898, 17693, 14728, 10412, 13436, 10088, 10189, 10218, 43528, 89, 19133, 10172, 43484, 10189, 19134, 10089, 10138, 12140, 11638, 13034, 11496, 10237, 10181, 43485, 10088, 58583, 10187, 43486, 10089, 10108, 640, 13053, 13198, 10079, 10782, 43425, 12642, 10127, 10257, 10114, 43435, 43358, 10079, 14288, 14728, 43347, 28329, 12642, 10127, 10257, 10114, 10139, 15260, 10199, 10177, 10185, 10189, 12133, 10138, 13241, 13849, 12061, 10132, 16403, 43448, 10125, 10080, 14215, 10370, 10138, 16572, 18142, 10132, 10079, 16821, 10616, 15643, 10139, 43314, 58526, 28329, 12642, 10127, 10257, 10114, 43328, 51, 19219, 52, 10768, 12341, 10079, 11024, 10257, 13188, 11638, 10138, 10237, 10181, 43485, 10108, 10794, 12354, 10260, 15752, 43410, 10159, 10079, 10148, 10152, 43399, 12883, 17058, 58534, 16451, 15586, 10132, 14894, 11595, 58534, 28329, 10222, 10189, 10179, 13651, 10139, 10079, 13004, 10175, 13190, 12341, 10136, 10703, 12752, 12616, 43352, 10133, 16417, 13078, 43347, 28329, 10189, 10218, 43528, 89, 10138, 10172, 10239, 10188, 10204, 10167, 10125, 10127, 10139, 10079, 43356, 43385, 43340, 13053, 10138, 10223, 10197, 58582, 10139, 12604, 10841, 58542, 58558, 43317, 10080, 10088, 13057, 13199, 10136, 17979, 15414, 10136, 16572, 18142, 43352, 10089, 43339, 10164, 11685, 10150, 10370, 10125, 10127, 43402, 10132, 10079, 14894, 11595, 10164, 12430, 58580, 43397, 10080, 13557, 10138, 10213, 43505, 10201, 10136, 11047, 10113, 10079, 11638, 17387, 10138, 10200, 43530, 10164, 10996, 17848, 43347, 10139, 10122, 10126, 10080, 51, 13521, 14778, 10138, 58583, 10187, 43486, 10139, 10079, 10147, 3022, 189, 18051, 16403, 13982, 10605, 10108, 13006, 64128, 28329, 10222, 10189, 10179, 13651, 10139, 10079, 10088, 33450, 89, 10196, 43534, 10138, 14735, 43344, 10079, 58583, 10187, 43486, 10138, 10172, 10177, 43468, 43487, 43519, 10135, 16572, 18142, 12642, 10127, 10257, 10114, 10079, 10106, 10151, 10132, 10133, 10104, 19126, 10078, 13004, 10175, 13190, 12341, 10136, 16403, 43448, 10160, 13557, 10138, 16572, 18142, 12642, 10127, 10257, 10114, 10136, 11047, 43337, 10079, 11632, 10111, 10164, 11871, 10165, 10126, 10089, 10133, 10198, 10169, 43531, 43347, 28329, 64137, 43383, 10237, 10181, 43485, 12642, 10127, 10257, 10114, 10164, 16587, 11021, 43352, 15260, 17197, 17269, 15878, 15090, 12005, 19133, 23940, 19134, 10139, 10079, 10336, 12985, 16605, 13299, 10164, 14773, 14819, 43352, 43388, 10125, 10080, 12212, 11549, 12701, 12307, 15643, 10139, 10079, 18051, 16403, 10285, 10136, 13521, 10452, 10108, 11645, 43448, 10125, 11550, 11038, 10138, 17189, 12182, 10138, 11961, 12380, 10126, 10133, 17162, 10146, 10125, 28329, 10222, 10189, 10179, 13651, 10139, 12642, 10127, 10257, 10114, 10808, 10079, 13199, 12335, 10567, 10164, 10258, 10114, 43429, 58558, 43310, 10080, 14728, 11965, 11022, 10138, 16552, 10605, 10164, 14894, 11595, 10123, 10122, 10136, 13521, 10452, 10164, 12642, 10127, 10257, 10114, 10160, 43366, 43385, 10088, 12604, 10841, 10089, 10126, 58558, 43310, 28329, 43355, 17988, 10103, 10139, 43327, 43369, 10080, 58583, 10187, 43486, 10139, 12642, 10127, 10257, 10114, 13038, 16552, 43328, 10134, 10165, 10134, 10165, 10257, 13074, 10119, 10079, 10225, 10177, 10187, 10183, 13974, 10138, 10257, 15090, 10145, 10133, 11047, 43326, 43372, 43369, 10080, 43346, 10119, 50, 10768, 10152, 10119, 43397, 10104, 10127, 10079, 10121, 10146, 10131, 10108, 16540, 14604, 17189, 10159, 10136, 17202, 43450, 43317, 43416, 43419, 10103, 58532, 13078, 10158, 10107, 58565, 10125, 10080]\n",
            "\n",
            "519 tokens\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "iyMoCykAL1g0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################################################################################################\n",
        "# Unit Test\n",
        "########################################################################################################\n",
        "\n",
        "print('Unit test...')\n",
        "\n",
        "QQQ = ['', ' ', 'Õ\\U000683b8', b'\\xe6\\xaa\\x81'.decode('utf-8')]\n",
        "\n",
        "for TRIAL in range(500):\n",
        "    x = ''\n",
        "    for xx in [\n",
        "        ['0',' '],\n",
        "        ['0','1'],\n",
        "        ['0','1',' '],\n",
        "        ['0','1',' ','00','11','  ','000','111','   '],\n",
        "        list('01 \\n\\r\\t,.;!?:\\'\\\"-=你好')\n",
        "    ]:\n",
        "        for i in range(256):\n",
        "            x += random.choice(xx)\n",
        "    QQQ += [x]\n",
        "\n",
        "for i in range(5000):\n",
        "    QQQ += [' ' * i]\n",
        "\n",
        "for TRIAL in range(5000):\n",
        "    x = chr(random.randrange(0, 256))\n",
        "    x = x * random.randrange(1, 32)\n",
        "    QQQ += [x]\n",
        "\n",
        "for TRIAL in range(99999):\n",
        "    x = chr(random.randrange(256, 1114112))\n",
        "    x = x * random.randrange(1, 4)\n",
        "    try:\n",
        "        tmp = x.encode(\"utf-8\")\n",
        "        QQQ += [x]\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4BSsx00LyEm",
        "outputId": "234fea50-5e28-4d78-a9fc-371f3cdcbe9d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unit test...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QQQ += ['''\n",
        "UTF-8 decoder capability and stress test\n",
        "----------------------------------------\n",
        "\n",
        "Markus Kuhn <http://www.cl.cam.ac.uk/~mgk25/> - 2015-08-28 - CC BY 4.0\n",
        "\n",
        "This test file can help you examine, how your UTF-8 decoder handles\n",
        "various types of correct, malformed, or otherwise interesting UTF-8\n",
        "sequences. This file is not meant to be a conformance test. It does\n",
        "not prescribe any particular outcome. Therefore, there is no way to\n",
        "\"pass\" or \"fail\" this test file, even though the text does suggest a\n",
        "preferable decoder behaviour at some places. Its aim is, instead, to\n",
        "help you think about, and test, the behaviour of your UTF-8 decoder on a\n",
        "systematic collection of unusual inputs. Experience so far suggests\n",
        "that most first-time authors of UTF-8 decoders find at least one\n",
        "serious problem in their decoder using this file.\n",
        "\n",
        "The test lines below cover boundary conditions, malformed UTF-8\n",
        "sequences, as well as correctly encoded UTF-8 sequences of Unicode code\n",
        "points that should never occur in a correct UTF-8 file.\n",
        "\n",
        "According to ISO 10646-1:2000, sections D.7 and 2.3c, a device\n",
        "receiving UTF-8 shall interpret a \"malformed sequence in the same way\n",
        "that it interprets a character that is outside the adopted subset\" and\n",
        "\"characters that are not within the adopted subset shall be indicated\n",
        "to the user\" by a receiving device. One commonly used approach in\n",
        "UTF-8 decoders is to replace any malformed UTF-8 sequence by a\n",
        "replacement character (U+FFFD), which looks a bit like an inverted\n",
        "question mark, or a similar symbol. It might be a good idea to\n",
        "visually distinguish a malformed UTF-8 sequence from a correctly\n",
        "encoded Unicode character that is just not available in the current\n",
        "font but otherwise fully legal, even though ISO 10646-1 doesn't\n",
        "mandate this. In any case, just ignoring malformed sequences or\n",
        "unavailable characters does not conform to ISO 10646, will make\n",
        "debugging more difficult, and can lead to user confusion.\n",
        "\n",
        "Please check, whether a malformed UTF-8 sequence is (1) represented at\n",
        "all, (2) represented by exactly one single replacement character (or\n",
        "equivalent signal), and (3) the following quotation mark after an\n",
        "illegal UTF-8 sequence is correctly displayed, i.e. proper\n",
        "resynchronization takes place immediately after any malformed\n",
        "sequence. This file says \"THE END\" in the last line, so if you don't\n",
        "see that, your decoder crashed somehow before, which should always be\n",
        "cause for concern.\n",
        "\n",
        "All lines in this file are exactly 79 characters long (plus the line\n",
        "feed). In addition, all lines end with \"|\", except for the two test\n",
        "lines 2.1.1 and 2.2.1, which contain non-printable ASCII controls\n",
        "U+0000 and U+007F. If you display this file with a fixed-width font,\n",
        "these \"|\" characters should all line up in column 79 (right margin).\n",
        "This allows you to test quickly, whether your UTF-8 decoder finds the\n",
        "correct number of characters in every line, that is whether each\n",
        "malformed sequences is replaced by a single replacement character.\n",
        "\n",
        "Note that, as an alternative to the notion of malformed sequence used\n",
        "here, it is also a perfectly acceptable (and in some situations even\n",
        "preferable) solution to represent each individual byte of a malformed\n",
        "sequence with a replacement character. If you follow this strategy in\n",
        "your decoder, then please ignore the \"|\" column.\n",
        "\n",
        "\n",
        "Here come the tests:                                                          |\n",
        "                                                                              |\n",
        "1  Some correct UTF-8 text                                                    |\n",
        "                                                                              |\n",
        "You should see the Greek word 'kosme':       \"κόσμε\"                          |\n",
        "                                                                              |\n",
        "2  Boundary condition test cases                                              |\n",
        "                                                                              |\n",
        "2.1  First possible sequence of a certain length                              |\n",
        "                                                                              |\n",
        "2.1.1  1 byte  (U-00000000):        \"�\"\n",
        "2.1.2  2 bytes (U-00000080):        \"\"                                       |\n",
        "2.1.3  3 bytes (U-00000800):        \"ࠀ\"                                       |\n",
        "2.1.4  4 bytes (U-00010000):        \"𐀀\"                                       |\n",
        "2.1.5  5 bytes (U-00200000):        \"�����\"                                       |\n",
        "2.1.6  6 bytes (U-04000000):        \"������\"                                       |\n",
        "                                                                              |\n",
        "2.2  Last possible sequence of a certain length                               |\n",
        "                                                                              |\n",
        "2.2.1  1 byte  (U-0000007F):        \"\"\n",
        "2.2.2  2 bytes (U-000007FF):        \"߿\"                                       |\n",
        "2.2.3  3 bytes (U-0000FFFF):        \"￿\"                                       |\n",
        "2.2.4  4 bytes (U-001FFFFF):        \"����\"                                       |\n",
        "2.2.5  5 bytes (U-03FFFFFF):        \"�����\"                                       |\n",
        "2.2.6  6 bytes (U-7FFFFFFF):        \"������\"                                       |\n",
        "                                                                              |\n",
        "2.3  Other boundary conditions                                                |\n",
        "                                                                              |\n",
        "2.3.1  U-0000D7FF = ed 9f bf = \"퟿\"                                            |\n",
        "2.3.2  U-0000E000 = ee 80 80 = \"\"                                            |\n",
        "2.3.3  U-0000FFFD = ef bf bd = \"�\"                                            |\n",
        "2.3.4  U-0010FFFF = f4 8f bf bf = \"􏿿\"                                         |\n",
        "2.3.5  U-00110000 = f4 90 80 80 = \"����\"                                         |\n",
        "                                                                              |\n",
        "3  Malformed sequences                                                        |\n",
        "                                                                              |\n",
        "3.1  Unexpected continuation bytes                                            |\n",
        "                                                                              |\n",
        "Each unexpected continuation byte should be separately signalled as a         |\n",
        "malformed sequence of its own.                                                |\n",
        "                                                                              |\n",
        "3.1.1  First continuation byte 0x80: \"�\"                                      |\n",
        "3.1.2  Last  continuation byte 0xbf: \"�\"                                      |\n",
        "                                                                              |\n",
        "3.1.3  2 continuation bytes: \"��\"                                             |\n",
        "3.1.4  3 continuation bytes: \"���\"                                            |\n",
        "3.1.5  4 continuation bytes: \"����\"                                           |\n",
        "3.1.6  5 continuation bytes: \"�����\"                                          |\n",
        "3.1.7  6 continuation bytes: \"������\"                                         |\n",
        "3.1.8  7 continuation bytes: \"�������\"                                        |\n",
        "                                                                              |\n",
        "3.1.9  Sequence of all 64 possible continuation bytes (0x80-0xbf):            |\n",
        "                                                                              |\n",
        "   \"����������������                                                          |\n",
        "    ����������������                                                          |\n",
        "    ����������������                                                          |\n",
        "    ����������������\"                                                         |\n",
        "                                                                              |\n",
        "3.2  Lonely start characters                                                  |\n",
        "                                                                              |\n",
        "3.2.1  All 32 first bytes of 2-byte sequences (0xc0-0xdf),                    |\n",
        "       each followed by a space character:                                    |\n",
        "                                                                              |\n",
        "   \"� � � � � � � � � � � � � � � �                                           |\n",
        "    � � � � � � � � � � � � � � � � \"                                         |\n",
        "                                                                              |\n",
        "3.2.2  All 16 first bytes of 3-byte sequences (0xe0-0xef),                    |\n",
        "       each followed by a space character:                                    |\n",
        "                                                                              |\n",
        "   \"� � � � � � � � � � � � � � � � \"                                         |\n",
        "                                                                              |\n",
        "3.2.3  All 8 first bytes of 4-byte sequences (0xf0-0xf7),                     |\n",
        "       each followed by a space character:                                    |\n",
        "                                                                              |\n",
        "   \"� � � � � � � � \"                                                         |\n",
        "                                                                              |\n",
        "3.2.4  All 4 first bytes of 5-byte sequences (0xf8-0xfb),                     |\n",
        "       each followed by a space character:                                    |\n",
        "                                                                              |\n",
        "   \"� � � � \"                                                                 |\n",
        "                                                                              |\n",
        "3.2.5  All 2 first bytes of 6-byte sequences (0xfc-0xfd),                     |\n",
        "       each followed by a space character:                                    |\n",
        "                                                                              |\n",
        "   \"� � \"                                                                     |\n",
        "                                                                              |\n",
        "3.3  Sequences with last continuation byte missing                            |\n",
        "                                                                              |\n",
        "All bytes of an incomplete sequence should be signalled as a single           |\n",
        "malformed sequence, i.e., you should see only a single replacement            |\n",
        "character in each of the next 10 tests. (Characters as in section 2)          |\n",
        "                                                                              |\n",
        "3.3.1  2-byte sequence with last byte missing (U+0000):     \"�\"               |\n",
        "3.3.2  3-byte sequence with last byte missing (U+0000):     \"��\"               |\n",
        "3.3.3  4-byte sequence with last byte missing (U+0000):     \"���\"               |\n",
        "3.3.4  5-byte sequence with last byte missing (U+0000):     \"����\"               |\n",
        "3.3.5  6-byte sequence with last byte missing (U+0000):     \"�����\"               |\n",
        "3.3.6  2-byte sequence with last byte missing (U-000007FF): \"�\"               |\n",
        "3.3.7  3-byte sequence with last byte missing (U-0000FFFF): \"�\"               |\n",
        "3.3.8  4-byte sequence with last byte missing (U-001FFFFF): \"���\"               |\n",
        "3.3.9  5-byte sequence with last byte missing (U-03FFFFFF): \"����\"               |\n",
        "3.3.10 6-byte sequence with last byte missing (U-7FFFFFFF): \"�����\"               |\n",
        "                                                                              |\n",
        "3.4  Concatenation of incomplete sequences                                    |\n",
        "                                                                              |\n",
        "All the 10 sequences of 3.3 concatenated, you should see 10 malformed         |\n",
        "sequences being signalled:                                                    |\n",
        "                                                                              |\n",
        "   \"�����������������������������\"                                                               |\n",
        "                                                                              |\n",
        "3.5  Impossible bytes                                                         |\n",
        "                                                                              |\n",
        "The following two bytes cannot appear in a correct UTF-8 string               |\n",
        "                                                                              |\n",
        "3.5.1  fe = \"�\"                                                               |\n",
        "3.5.2  ff = \"�\"                                                               |\n",
        "3.5.3  fe fe ff ff = \"����\"                                                   |\n",
        "                                                                              |\n",
        "4  Overlong sequences                                                         |\n",
        "                                                                              |\n",
        "The following sequences are not malformed according to the letter of          |\n",
        "the Unicode 2.0 standard. However, they are longer then necessary and         |\n",
        "a correct UTF-8 encoder is not allowed to produce them. A \"safe UTF-8         |\n",
        "decoder\" should reject them just like malformed sequences for two             |\n",
        "reasons: (1) It helps to debug applications if overlong sequences are         |\n",
        "not treated as valid representations of characters, because this helps        |\n",
        "to spot problems more quickly. (2) Overlong sequences provide                 |\n",
        "alternative representations of characters, that could maliciously be          |\n",
        "used to bypass filters that check only for ASCII characters. For              |\n",
        "instance, a 2-byte encoded line feed (LF) would not be caught by a            |\n",
        "line counter that counts only 0x0a bytes, but it would still be               |\n",
        "processed as a line feed by an unsafe UTF-8 decoder later in the              |\n",
        "pipeline. From a security point of view, ASCII compatibility of UTF-8         |\n",
        "sequences means also, that ASCII characters are *only* allowed to be          |\n",
        "represented by ASCII bytes in the range 0x00-0x7f. To ensure this             |\n",
        "aspect of ASCII compatibility, use only \"safe UTF-8 decoders\" that            |\n",
        "reject overlong UTF-8 sequences for which a shorter encoding exists.          |\n",
        "                                                                              |\n",
        "4.1  Examples of an overlong ASCII character                                  |\n",
        "                                                                              |\n",
        "With a safe UTF-8 decoder, all of the following five overlong                 |\n",
        "representations of the ASCII character slash (\"/\") should be rejected         |\n",
        "like a malformed UTF-8 sequence, for instance by substituting it with         |\n",
        "a replacement character. If you see a slash below, you do not have a          |\n",
        "safe UTF-8 decoder!                                                           |\n",
        "                                                                              |\n",
        "4.1.1 U+002F = c0 af             = \"��\"                                        |\n",
        "4.1.2 U+002F = e0 80 af          = \"���\"                                        |\n",
        "4.1.3 U+002F = f0 80 80 af       = \"����\"                                        |\n",
        "4.1.4 U+002F = f8 80 80 80 af    = \"�����\"                                        |\n",
        "4.1.5 U+002F = fc 80 80 80 80 af = \"������\"                                        |\n",
        "                                                                              |\n",
        "4.2  Maximum overlong sequences                                               |\n",
        "                                                                              |\n",
        "Below you see the highest Unicode value that is still resulting in an         |\n",
        "overlong sequence if represented with the given number of bytes. This         |\n",
        "is a boundary test for safe UTF-8 decoders. All five characters should        |\n",
        "be rejected like malformed UTF-8 sequences.                                   |\n",
        "                                                                              |\n",
        "4.2.1  U-0000007F = c1 bf             = \"��\"                                   |\n",
        "4.2.2  U-000007FF = e0 9f bf          = \"���\"                                   |\n",
        "4.2.3  U-0000FFFF = f0 8f bf bf       = \"����\"                                   |\n",
        "4.2.4  U-001FFFFF = f8 87 bf bf bf    = \"�����\"                                   |\n",
        "4.2.5  U-03FFFFFF = fc 83 bf bf bf bf = \"������\"                                   |\n",
        "                                                                              |\n",
        "4.3  Overlong representation of the NUL character                             |\n",
        "                                                                              |\n",
        "The following five sequences should also be rejected like malformed           |\n",
        "UTF-8 sequences and should not be treated like the ASCII NUL                  |\n",
        "character.                                                                    |\n",
        "                                                                              |\n",
        "4.3.1  U+0000 = c0 80             = \"��\"                                       |\n",
        "4.3.2  U+0000 = e0 80 80          = \"���\"                                       |\n",
        "4.3.3  U+0000 = f0 80 80 80       = \"����\"                                       |\n",
        "4.3.4  U+0000 = f8 80 80 80 80    = \"�����\"                                       |\n",
        "4.3.5  U+0000 = fc 80 80 80 80 80 = \"������\"                                       |\n",
        "                                                                              |\n",
        "5  Illegal code positions                                                     |\n",
        "                                                                              |\n",
        "The following UTF-8 sequences should be rejected like malformed               |\n",
        "sequences, because they never represent valid ISO 10646 characters and        |\n",
        "a UTF-8 decoder that accepts them might introduce security problems           |\n",
        "comparable to overlong UTF-8 sequences.                                       |\n",
        "                                                                              |\n",
        "5.1 Single UTF-16 surrogates                                                  |\n",
        "                                                                              |\n",
        "5.1.1  U+D800 = ed a0 80 = \"���\"                                                |\n",
        "5.1.2  U+DB7F = ed ad bf = \"���\"                                                |\n",
        "5.1.3  U+DB80 = ed ae 80 = \"���\"                                                |\n",
        "5.1.4  U+DBFF = ed af bf = \"���\"                                                |\n",
        "5.1.5  U+DC00 = ed b0 80 = \"���\"                                                |\n",
        "5.1.6  U+DF80 = ed be 80 = \"���\"                                                |\n",
        "5.1.7  U+DFFF = ed bf bf = \"���\"                                                |\n",
        "                                                                              |\n",
        "5.2 Paired UTF-16 surrogates                                                  |\n",
        "                                                                              |\n",
        "5.2.1  U+D800 U+DC00 = ed a0 80 ed b0 80 = \"������\"                               |\n",
        "5.2.2  U+D800 U+DFFF = ed a0 80 ed bf bf = \"������\"                               |\n",
        "5.2.3  U+DB7F U+DC00 = ed ad bf ed b0 80 = \"������\"                               |\n",
        "5.2.4  U+DB7F U+DFFF = ed ad bf ed bf bf = \"������\"                               |\n",
        "5.2.5  U+DB80 U+DC00 = ed ae 80 ed b0 80 = \"������\"                               |\n",
        "5.2.6  U+DB80 U+DFFF = ed ae 80 ed bf bf = \"������\"                               |\n",
        "5.2.7  U+DBFF U+DC00 = ed af bf ed b0 80 = \"������\"                               |\n",
        "5.2.8  U+DBFF U+DFFF = ed af bf ed bf bf = \"������\"                               |\n",
        "                                                                              |\n",
        "5.3 Noncharacter code positions                                               |\n",
        "                                                                              |\n",
        "The following \"noncharacters\" are \"reserved for internal use\" by              |\n",
        "applications, and according to older versions of the Unicode Standard         |\n",
        "\"should never be interchanged\". Unicode Corrigendum #9 dropped the            |\n",
        "latter restriction. Nevertheless, their presence in incoming UTF-8 data       |\n",
        "can remain a potential security risk, depending on what use is made of        |\n",
        "these codes subsequently. Examples of such internal use:                      |\n",
        "                                                                              |\n",
        " - Some file APIs with 16-bit characters may use the integer value -1         |\n",
        "   = U+FFFF to signal an end-of-file (EOF) or error condition.                |\n",
        "                                                                              |\n",
        " - In some UTF-16 receivers, code point U+FFFE might trigger a                |\n",
        "   byte-swap operation (to convert between UTF-16LE and UTF-16BE).            |\n",
        "                                                                              |\n",
        "With such internal use of noncharacters, it may be desirable and safer        |\n",
        "to block those code points in UTF-8 decoders, as they should never            |\n",
        "occur legitimately in incoming UTF-8 data, and could trigger unsafe           |\n",
        "behaviour in subsequent processing.                                           |\n",
        "                                                                              |\n",
        "Particularly problematic noncharacters in 16-bit applications:                |\n",
        "                                                                              |\n",
        "5.3.1  U+FFFE = ef bf be = \"￾\"                                                |\n",
        "5.3.2  U+FFFF = ef bf bf = \"￿\"                                                |\n",
        "                                                                              |\n",
        "Other noncharacters:                                                          |\n",
        "                                                                              |\n",
        "5.3.3  U+FDD0 .. U+FDEF = \"﷐﷑﷒﷓﷔﷕﷖﷗﷘﷙﷚﷛﷜﷝﷞﷟﷠﷡﷢﷣﷤﷥﷦﷧﷨﷩﷪﷫﷬﷭﷮﷯\"|\n",
        "                                                                              |\n",
        "5.3.4  U+nFFFE U+nFFFF (for n = 1..10)                                        |\n",
        "                                                                              |\n",
        "       \"🿾🿿𯿾𯿿𿿾𿿿񏿾񏿿񟿾񟿿񯿾񯿿񿿾񿿿򏿾򏿿                                    |\n",
        "        򟿾򟿿򯿾򯿿򿿾򿿿󏿾󏿿󟿾󟿿󯿾󯿿󿿾󿿿􏿾􏿿\"                                   |\n",
        "                                                                              |\n",
        "THE END                                                                       |\n",
        "\n",
        "\n",
        "UTF-8 encoded sample plain-text file\n",
        "‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾\n",
        "\n",
        "Markus Kuhn [ˈmaʳkʊs kuːn] <http://www.cl.cam.ac.uk/~mgk25/> — 2002-07-25 CC BY\n",
        "\n",
        "\n",
        "The ASCII compatible UTF-8 encoding used in this plain-text file\n",
        "is defined in Unicode, ISO 10646-1, and RFC 2279.\n",
        "\n",
        "\n",
        "Using Unicode/UTF-8, you can write in emails and source code things such as\n",
        "\n",
        "Mathematics and sciences:\n",
        "\n",
        "  ∮ E⋅da = Q,  n → ∞, ∑ f(i) = ∏ g(i),      ⎧⎡⎛┌─────┐⎞⎤⎫\n",
        "                                            ⎪⎢⎜│a²+b³ ⎟⎥⎪\n",
        "  ∀x∈ℝ: ⌈x⌉ = −⌊−x⌋, α ∧ ¬β = ¬(¬α ∨ β),    ⎪⎢⎜│───── ⎟⎥⎪\n",
        "                                            ⎪⎢⎜⎷ c₈   ⎟⎥⎪\n",
        "  ℕ ⊆ ℕ₀ ⊂ ℤ ⊂ ℚ ⊂ ℝ ⊂ ℂ,                   ⎨⎢⎜       ⎟⎥⎬\n",
        "                                            ⎪⎢⎜ ∞     ⎟⎥⎪\n",
        "  ⊥ < a ≠ b ≡ c ≤ d ≪ ⊤ ⇒ (⟦A⟧ ⇔ ⟪B⟫),      ⎪⎢⎜ ⎲     ⎟⎥⎪\n",
        "                                            ⎪⎢⎜ ⎳aⁱ-bⁱ⎟⎥⎪\n",
        "  2H₂ + O₂ ⇌ 2H₂O, R = 4.7 kΩ, ⌀ 200 mm     ⎩⎣⎝i=1    ⎠⎦⎭\n",
        "\n",
        "Linguistics and dictionaries:\n",
        "\n",
        "  ði ıntəˈnæʃənəl fəˈnɛtık əsoʊsiˈeıʃn\n",
        "  Y [ˈʏpsilɔn], Yen [jɛn], Yoga [ˈjoːgɑ]\n",
        "\n",
        "APL:\n",
        "\n",
        "  ((V⍳V)=⍳⍴V)/V←,V    ⌷←⍳→⍴∆∇⊃‾⍎⍕⌈\n",
        "\n",
        "Nicer typography in plain text files:\n",
        "\n",
        "  ╔══════════════════════════════════════════╗\n",
        "  ║                                          ║\n",
        "  ║   • ‘single’ and “double” quotes         ║\n",
        "  ║                                          ║\n",
        "  ║   • Curly apostrophes: “We’ve been here” ║\n",
        "  ║                                          ║\n",
        "  ║   • Latin-1 apostrophe and accents: '´`  ║\n",
        "  ║                                          ║\n",
        "  ║   • ‚deutsche‘ „Anführungszeichen“       ║\n",
        "  ║                                          ║\n",
        "  ║   • †, ‡, ‰, •, 3–4, —, −5/+5, ™, …      ║\n",
        "  ║                                          ║\n",
        "  ║   • ASCII safety test: 1lI|, 0OD, 8B     ║\n",
        "  ║                      ╭─────────╮         ║\n",
        "  ║   • the euro symbol: │ 14.95 € │         ║\n",
        "  ║                      ╰─────────╯         ║\n",
        "  ╚══════════════════════════════════════════╝\n",
        "\n",
        "Combining characters:\n",
        "\n",
        "  STARGΛ̊TE SG-1, a = v̇ = r̈, a⃑ ⊥ b⃑\n",
        "\n",
        "Greek (in Polytonic):\n",
        "\n",
        "  The Greek anthem:\n",
        "\n",
        "  Σὲ γνωρίζω ἀπὸ τὴν κόψη\n",
        "  τοῦ σπαθιοῦ τὴν τρομερή,\n",
        "  σὲ γνωρίζω ἀπὸ τὴν ὄψη\n",
        "  ποὺ μὲ βία μετράει τὴ γῆ.\n",
        "\n",
        "  ᾿Απ᾿ τὰ κόκκαλα βγαλμένη\n",
        "  τῶν ῾Ελλήνων τὰ ἱερά\n",
        "  καὶ σὰν πρῶτα ἀνδρειωμένη\n",
        "  χαῖρε, ὦ χαῖρε, ᾿Ελευθεριά!\n",
        "\n",
        "  From a speech of Demosthenes in the 4th century BC:\n",
        "\n",
        "  Οὐχὶ ταὐτὰ παρίσταταί μοι γιγνώσκειν, ὦ ἄνδρες ᾿Αθηναῖοι,\n",
        "  ὅταν τ᾿ εἰς τὰ πράγματα ἀποβλέψω καὶ ὅταν πρὸς τοὺς\n",
        "  λόγους οὓς ἀκούω· τοὺς μὲν γὰρ λόγους περὶ τοῦ\n",
        "  τιμωρήσασθαι Φίλιππον ὁρῶ γιγνομένους, τὰ δὲ πράγματ᾿\n",
        "  εἰς τοῦτο προήκοντα,  ὥσθ᾿ ὅπως μὴ πεισόμεθ᾿ αὐτοὶ\n",
        "  πρότερον κακῶς σκέψασθαι δέον. οὐδέν οὖν ἄλλο μοι δοκοῦσιν\n",
        "  οἱ τὰ τοιαῦτα λέγοντες ἢ τὴν ὑπόθεσιν, περὶ ἧς βουλεύεσθαι,\n",
        "  οὐχὶ τὴν οὖσαν παριστάντες ὑμῖν ἁμαρτάνειν. ἐγὼ δέ, ὅτι μέν\n",
        "  ποτ᾿ ἐξῆν τῇ πόλει καὶ τὰ αὑτῆς ἔχειν ἀσφαλῶς καὶ Φίλιππον\n",
        "  τιμωρήσασθαι, καὶ μάλ᾿ ἀκριβῶς οἶδα· ἐπ᾿ ἐμοῦ γάρ, οὐ πάλαι\n",
        "  γέγονεν ταῦτ᾿ ἀμφότερα· νῦν μέντοι πέπεισμαι τοῦθ᾿ ἱκανὸν\n",
        "  προλαβεῖν ἡμῖν εἶναι τὴν πρώτην, ὅπως τοὺς συμμάχους\n",
        "  σώσομεν. ἐὰν γὰρ τοῦτο βεβαίως ὑπάρξῃ, τότε καὶ περὶ τοῦ\n",
        "  τίνα τιμωρήσεταί τις καὶ ὃν τρόπον ἐξέσται σκοπεῖν· πρὶν δὲ\n",
        "  τὴν ἀρχὴν ὀρθῶς ὑποθέσθαι, μάταιον ἡγοῦμαι περὶ τῆς\n",
        "  τελευτῆς ὁντινοῦν ποιεῖσθαι λόγον.\n",
        "\n",
        "  Δημοσθένους, Γ´ ᾿Ολυνθιακὸς\n",
        "\n",
        "Georgian:\n",
        "\n",
        "  From a Unicode conference invitation:\n",
        "\n",
        "  გთხოვთ ახლავე გაიაროთ რეგისტრაცია Unicode-ის მეათე საერთაშორისო\n",
        "  კონფერენციაზე დასასწრებად, რომელიც გაიმართება 10-12 მარტს,\n",
        "  ქ. მაინცში, გერმანიაში. კონფერენცია შეჰკრებს ერთად მსოფლიოს\n",
        "  ექსპერტებს ისეთ დარგებში როგორიცაა ინტერნეტი და Unicode-ი,\n",
        "  ინტერნაციონალიზაცია და ლოკალიზაცია, Unicode-ის გამოყენება\n",
        "  ოპერაციულ სისტემებსა, და გამოყენებით პროგრამებში, შრიფტებში,\n",
        "  ტექსტების დამუშავებასა და მრავალენოვან კომპიუტერულ სისტემებში.\n",
        "\n",
        "Russian:\n",
        "\n",
        "  From a Unicode conference invitation:\n",
        "\n",
        "  Зарегистрируйтесь сейчас на Десятую Международную Конференцию по\n",
        "  Unicode, которая состоится 10-12 марта 1997 года в Майнце в Германии.\n",
        "  Конференция соберет широкий круг экспертов по  вопросам глобального\n",
        "  Интернета и Unicode, локализации и интернационализации, воплощению и\n",
        "  применению Unicode в различных операционных системах и программных\n",
        "  приложениях, шрифтах, верстке и многоязычных компьютерных системах.\n",
        "\n",
        "Thai (UCS Level 2):\n",
        "\n",
        "  Excerpt from a poetry on The Romance of The Three Kingdoms (a Chinese\n",
        "  classic 'San Gua'):\n",
        "\n",
        "  [----------------------------|------------------------]\n",
        "    ๏ แผ่นดินฮั่นเสื่อมโทรมแสนสังเวช  พระปกเกศกองบู๊กู้ขึ้นใหม่\n",
        "  สิบสองกษัตริย์ก่อนหน้าแลถัดไป       สององค์ไซร้โง่เขลาเบาปัญญา\n",
        "    ทรงนับถือขันทีเป็นที่พึ่ง           บ้านเมืองจึงวิปริตเป็นนักหนา\n",
        "  โฮจิ๋นเรียกทัพทั่วหัวเมืองมา         หมายจะฆ่ามดชั่วตัวสำคัญ\n",
        "    เหมือนขับไสไล่เสือจากเคหา      รับหมาป่าเข้ามาเลยอาสัญ\n",
        "  ฝ่ายอ้องอุ้นยุแยกให้แตกกัน          ใช้สาวนั้นเป็นชนวนชื่นชวนใจ\n",
        "    พลันลิฉุยกุยกีกลับก่อเหตุ          ช่างอาเพศจริงหนาฟ้าร้องไห้\n",
        "  ต้องรบราฆ่าฟันจนบรรลัย           ฤๅหาใครค้ำชูกู้บรรลังก์ ฯ\n",
        "\n",
        "  (The above is a two-column text. If combining characters are handled\n",
        "  correctly, the lines of the second column should be aligned with the\n",
        "  | character above.)\n",
        "\n",
        "Ethiopian:\n",
        "\n",
        "  Proverbs in the Amharic language:\n",
        "\n",
        "  ሰማይ አይታረስ ንጉሥ አይከሰስ።\n",
        "  ብላ ካለኝ እንደአባቴ በቆመጠኝ።\n",
        "  ጌጥ ያለቤቱ ቁምጥና ነው።\n",
        "  ደሀ በሕልሙ ቅቤ ባይጠጣ ንጣት በገደለው።\n",
        "  የአፍ ወለምታ በቅቤ አይታሽም።\n",
        "  አይጥ በበላ ዳዋ ተመታ።\n",
        "  ሲተረጉሙ ይደረግሙ።\n",
        "  ቀስ በቀስ፥ ዕንቁላል በእግሩ ይሄዳል።\n",
        "  ድር ቢያብር አንበሳ ያስር።\n",
        "  ሰው እንደቤቱ እንጅ እንደ ጉረቤቱ አይተዳደርም።\n",
        "  እግዜር የከፈተውን ጉሮሮ ሳይዘጋው አይድርም።\n",
        "  የጎረቤት ሌባ፥ ቢያዩት ይስቅ ባያዩት ያጠልቅ።\n",
        "  ሥራ ከመፍታት ልጄን ላፋታት።\n",
        "  ዓባይ ማደሪያ የለው፥ ግንድ ይዞ ይዞራል።\n",
        "  የእስላም አገሩ መካ የአሞራ አገሩ ዋርካ።\n",
        "  ተንጋሎ ቢተፉ ተመልሶ ባፉ።\n",
        "  ወዳጅህ ማር ቢሆን ጨርስህ አትላሰው።\n",
        "  እግርህን በፍራሽህ ልክ ዘርጋ።\n",
        "\n",
        "Runes:\n",
        "\n",
        "  ᚻᛖ ᚳᚹᚫᚦ ᚦᚫᛏ ᚻᛖ ᛒᚢᛞᛖ ᚩᚾ ᚦᚫᛗ ᛚᚪᚾᛞᛖ ᚾᚩᚱᚦᚹᛖᚪᚱᛞᚢᛗ ᚹᛁᚦ ᚦᚪ ᚹᛖᛥᚫ\n",
        "\n",
        "  (Old English, which transcribed into Latin reads 'He cwaeth that he\n",
        "  bude thaem lande northweardum with tha Westsae.' and means 'He said\n",
        "  that he lived in the northern land near the Western Sea.')\n",
        "\n",
        "Braille:\n",
        "\n",
        "  ⡌⠁⠧⠑ ⠼⠁⠒  ⡍⠜⠇⠑⠹⠰⠎ ⡣⠕⠌\n",
        "\n",
        "  ⡍⠜⠇⠑⠹ ⠺⠁⠎ ⠙⠑⠁⠙⠒ ⠞⠕ ⠃⠑⠛⠔ ⠺⠊⠹⠲ ⡹⠻⠑ ⠊⠎ ⠝⠕ ⠙⠳⠃⠞\n",
        "  ⠱⠁⠞⠑⠧⠻ ⠁⠃⠳⠞ ⠹⠁⠞⠲ ⡹⠑ ⠗⠑⠛⠊⠌⠻ ⠕⠋ ⠙⠊⠎ ⠃⠥⠗⠊⠁⠇ ⠺⠁⠎\n",
        "  ⠎⠊⠛⠝⠫ ⠃⠹ ⠹⠑ ⠊⠇⠻⠛⠹⠍⠁⠝⠂ ⠹⠑ ⠊⠇⠻⠅⠂ ⠹⠑ ⠥⠝⠙⠻⠞⠁⠅⠻⠂\n",
        "  ⠁⠝⠙ ⠹⠑ ⠡⠊⠑⠋ ⠍⠳⠗⠝⠻⠲ ⡎⠊⠗⠕⠕⠛⠑ ⠎⠊⠛⠝⠫ ⠊⠞⠲ ⡁⠝⠙\n",
        "  ⡎⠊⠗⠕⠕⠛⠑⠰⠎ ⠝⠁⠍⠑ ⠺⠁⠎ ⠛⠕⠕⠙ ⠥⠏⠕⠝ ⠰⡡⠁⠝⠛⠑⠂ ⠋⠕⠗ ⠁⠝⠹⠹⠔⠛ ⠙⠑\n",
        "  ⠡⠕⠎⠑ ⠞⠕ ⠏⠥⠞ ⠙⠊⠎ ⠙⠁⠝⠙ ⠞⠕⠲\n",
        "\n",
        "  ⡕⠇⠙ ⡍⠜⠇⠑⠹ ⠺⠁⠎ ⠁⠎ ⠙⠑⠁⠙ ⠁⠎ ⠁ ⠙⠕⠕⠗⠤⠝⠁⠊⠇⠲\n",
        "\n",
        "  ⡍⠔⠙⠖ ⡊ ⠙⠕⠝⠰⠞ ⠍⠑⠁⠝ ⠞⠕ ⠎⠁⠹ ⠹⠁⠞ ⡊ ⠅⠝⠪⠂ ⠕⠋ ⠍⠹\n",
        "  ⠪⠝ ⠅⠝⠪⠇⠫⠛⠑⠂ ⠱⠁⠞ ⠹⠻⠑ ⠊⠎ ⠏⠜⠞⠊⠊⠥⠇⠜⠇⠹ ⠙⠑⠁⠙ ⠁⠃⠳⠞\n",
        "  ⠁ ⠙⠕⠕⠗⠤⠝⠁⠊⠇⠲ ⡊ ⠍⠊⠣⠞ ⠙⠁⠧⠑ ⠃⠑⠲ ⠔⠊⠇⠔⠫⠂ ⠍⠹⠎⠑⠇⠋⠂ ⠞⠕\n",
        "  ⠗⠑⠛⠜⠙ ⠁ ⠊⠕⠋⠋⠔⠤⠝⠁⠊⠇ ⠁⠎ ⠹⠑ ⠙⠑⠁⠙⠑⠌ ⠏⠊⠑⠊⠑ ⠕⠋ ⠊⠗⠕⠝⠍⠕⠝⠛⠻⠹\n",
        "  ⠔ ⠹⠑ ⠞⠗⠁⠙⠑⠲ ⡃⠥⠞ ⠹⠑ ⠺⠊⠎⠙⠕⠍ ⠕⠋ ⠳⠗ ⠁⠝⠊⠑⠌⠕⠗⠎\n",
        "  ⠊⠎ ⠔ ⠹⠑ ⠎⠊⠍⠊⠇⠑⠆ ⠁⠝⠙ ⠍⠹ ⠥⠝⠙⠁⠇⠇⠪⠫ ⠙⠁⠝⠙⠎\n",
        "  ⠩⠁⠇⠇ ⠝⠕⠞ ⠙⠊⠌⠥⠗⠃ ⠊⠞⠂ ⠕⠗ ⠹⠑ ⡊⠳⠝⠞⠗⠹⠰⠎ ⠙⠕⠝⠑ ⠋⠕⠗⠲ ⡹⠳\n",
        "  ⠺⠊⠇⠇ ⠹⠻⠑⠋⠕⠗⠑ ⠏⠻⠍⠊⠞ ⠍⠑ ⠞⠕ ⠗⠑⠏⠑⠁⠞⠂ ⠑⠍⠏⠙⠁⠞⠊⠊⠁⠇⠇⠹⠂ ⠹⠁⠞\n",
        "  ⡍⠜⠇⠑⠹ ⠺⠁⠎ ⠁⠎ ⠙⠑⠁⠙ ⠁⠎ ⠁ ⠙⠕⠕⠗⠤⠝⠁⠊⠇⠲\n",
        "\n",
        "  (The first couple of paragraphs of \"A Christmas Carol\" by Dickens)\n",
        "\n",
        "Compact font selection example text:\n",
        "\n",
        "  ABCDEFGHIJKLMNOPQRSTUVWXYZ /0123456789\n",
        "  abcdefghijklmnopqrstuvwxyz £©µÀÆÖÞßéöÿ\n",
        "  –—‘“”„†•…‰™œŠŸž€ ΑΒΓΔΩαβγδω АБВГДабвгд\n",
        "  ∀∂∈ℝ∧∪≡∞ ↑↗↨↻⇣ ┐┼╔╘░►☺♀ ﬁ�⑀₂ἠḂӥẄɐː⍎אԱა\n",
        "\n",
        "Greetings in various languages:\n",
        "\n",
        "  Hello world, Καλημέρα κόσμε, コンニチハ\n",
        "\n",
        "Box drawing alignment tests:                                          █\n",
        "                                                                      ▉\n",
        "  ╔══╦══╗  ┌──┬──┐  ╭──┬──╮  ╭──┬──╮  ┏━━┳━━┓  ┎┒┏┑   ╷  ╻ ┏┯┓ ┌┰┐    ▊ ╱╲╱╲╳╳╳\n",
        "  ║┌─╨─┐║  │╔═╧═╗│  │╒═╪═╕│  │╓─╁─╖│  ┃┌─╂─┐┃  ┗╃╄┙  ╶┼╴╺╋╸┠┼┨ ┝╋┥    ▋ ╲╱╲╱╳╳╳\n",
        "  ║│╲ ╱│║  │║   ║│  ││ │ ││  │║ ┃ ║│  ┃│ ╿ │┃  ┍╅╆┓   ╵  ╹ ┗┷┛ └┸┘    ▌ ╱╲╱╲╳╳╳\n",
        "  ╠╡ ╳ ╞╣  ├╢   ╟┤  ├┼─┼─┼┤  ├╫─╂─╫┤  ┣┿╾┼╼┿┫  ┕┛┖┚     ┌┄┄┐ ╎ ┏┅┅┓ ┋ ▍ ╲╱╲╱╳╳╳\n",
        "  ║│╱ ╲│║  │║   ║│  ││ │ ││  │║ ┃ ║│  ┃│ ╽ │┃  ░░▒▒▓▓██ ┊  ┆ ╎ ╏  ┇ ┋ ▎\n",
        "  ║└─╥─┘║  │╚═╤═╝│  │╘═╪═╛│  │╙─╀─╜│  ┃└─╂─┘┃  ░░▒▒▓▓██ ┊  ┆ ╎ ╏  ┇ ┋ ▏\n",
        "  ╚══╩══╝  └──┴──┘  ╰──┴──╯  ╰──┴──╯  ┗━━┻━━┛  ▗▄▖▛▀▜   └╌╌┘ ╎ ┗╍╍┛ ┋  ▁▂▃▄▅▆▇█\n",
        "                                               ▝▀▘▙▄▟\n",
        "\n",
        "Sanskrit: ﻿काचं शक्नोम्यत्तुम् । नोपहिनस्ति माम् ॥\n",
        "Sanskrit (standard transcription): kācaṃ śaknomyattum; nopahinasti mām.\n",
        "Classical Greek: ὕαλον ϕαγεῖν δύναμαι· τοῦτο οὔ με βλάπτει.\n",
        "Greek (monotonic): Μπορώ να φάω σπασμένα γυαλιά χωρίς να πάθω τίποτα.\n",
        "Greek (polytonic): Μπορῶ νὰ φάω σπασμένα γυαλιὰ χωρὶς νὰ πάθω τίποτα.\n",
        "Etruscan: (NEEDED)\n",
        "Latin: Vitrum edere possum; mihi non nocet.\n",
        "Old French: Je puis mangier del voirre. Ne me nuit.\n",
        "French: Je peux manger du verre, ça ne me fait pas mal.\n",
        "Provençal / Occitan: Pòdi manjar de veire, me nafrariá pas.\n",
        "Québécois: J'peux manger d'la vitre, ça m'fa pas mal.\n",
        "Walloon: Dji pou magnî do vêre, çoula m' freut nén må.\n",
        "Champenois: (NEEDED)\n",
        "Lorrain: (NEEDED)\n",
        "Picard: Ch'peux mingi du verre, cha m'foé mie n'ma.\n",
        "Corsican/Corsu: (NEEDED)\n",
        "Jèrriais: (NEEDED)\n",
        "Kreyòl Ayisyen (Haitï): Mwen kap manje vè, li pa blese'm.\n",
        "Basque: Kristala jan dezaket, ez dit minik ematen.\n",
        "Catalan / Català: Puc menjar vidre, que no em fa mal.\n",
        "Spanish: Puedo comer vidrio, no me hace daño.\n",
        "Aragonés: Puedo minchar beire, no me'n fa mal .\n",
        "Aranés: (NEEDED)\n",
        "Mallorquín: (NEEDED)\n",
        "Galician: Eu podo xantar cristais e non cortarme.\n",
        "European Portuguese: Posso comer vidro, não me faz mal.\n",
        "Brazilian Portuguese (8): Posso comer vidro, não me machuca.\n",
        "Caboverdiano/Kabuverdianu (Cape Verde): M' podê cumê vidru, ca ta maguâ-m'.\n",
        "Papiamentu: Ami por kome glas anto e no ta hasimi daño.\n",
        "Italian: Posso mangiare il vetro e non mi fa male.\n",
        "Milanese: Sôn bôn de magnà el véder, el me fa minga mal.\n",
        "Roman: Me posso magna' er vetro, e nun me fa male.\n",
        "Napoletano: M' pozz magna' o'vetr, e nun m' fa mal.\n",
        "Venetian: Mi posso magnare el vetro, no'l me fa mae.\n",
        "Zeneise (Genovese): Pòsso mangiâ o veddro e o no me fà mâ.\n",
        "Sicilian: Puotsu mangiari u vitru, nun mi fa mali.\n",
        "Campinadese (Sardinia): (NEEDED)\n",
        "Lugudorese (Sardinia): (NEEDED)\n",
        "Romansch (Grischun): Jau sai mangiar vaider, senza che quai fa donn a mai.\n",
        "Romany / Tsigane: (NEEDED)\n",
        "Romanian: Pot să mănânc sticlă și ea nu mă rănește.\n",
        "Esperanto: Mi povas manĝi vitron, ĝi ne damaĝas min.\n",
        "Pictish: (NEEDED)\n",
        "Breton: (NEEDED)\n",
        "Cornish: Mý a yl dybry gwéder hag éf ny wra ow ankenya.\n",
        "Welsh: Dw i'n gallu bwyta gwydr, 'dyw e ddim yn gwneud dolur i mi.\n",
        "Manx Gaelic: Foddym gee glonney agh cha jean eh gortaghey mee.\n",
        "Old Irish (Ogham): ᚛᚛ᚉᚑᚅᚔᚉᚉᚔᚋ ᚔᚈᚔ ᚍᚂᚐᚅᚑ ᚅᚔᚋᚌᚓᚅᚐ᚜\n",
        "Old Irish (Latin): Con·iccim ithi nglano. Ním·géna.\n",
        "Irish: Is féidir liom gloinne a ithe. Ní dhéanann sí dochar ar bith dom.\n",
        "Ulster Gaelic: Ithim-sa gloine agus ní miste damh é.\n",
        "Scottish Gaelic: S urrainn dhomh gloinne ithe; cha ghoirtich i mi.\n",
        "Anglo-Saxon (Runes): ᛁᚳ᛫ᛗᚨᚷ᛫ᚷᛚᚨᛋ᛫ᛖᚩᛏᚪᚾ᛫ᚩᚾᛞ᛫ᚻᛁᛏ᛫ᚾᛖ᛫ᚻᛖᚪᚱᛗᛁᚪᚧ᛫ᛗᛖ᛬\n",
        "Anglo-Saxon (Latin): Ic mæg glæs eotan ond hit ne hearmiað me.\n",
        "Middle English: Ich canne glas eten and hit hirtiþ me nouȝt.\n",
        "English: I can eat glass and it doesn't hurt me.\n",
        "English (IPA): [aɪ kæn iːt glɑːs ænd ɪt dɐz nɒt hɜːt miː] (Received Pronunciation)\n",
        "English (Braille): ⠊⠀⠉⠁⠝⠀⠑⠁⠞⠀⠛⠇⠁⠎⠎⠀⠁⠝⠙⠀⠊⠞⠀⠙⠕⠑⠎⠝⠞⠀⠓⠥⠗⠞⠀⠍⠑\n",
        "Jamaican: Mi kian niam glas han i neba hot mi.\n",
        "Lalland Scots / Doric: Ah can eat gless, it disnae hurt us.\n",
        "Glaswegian: (NEEDED)\n",
        "Gothic (4): 𐌼𐌰𐌲 𐌲𐌻𐌴𐍃 𐌹̈𐍄𐌰𐌽, 𐌽𐌹 𐌼𐌹𐍃 𐍅𐌿 𐌽𐌳𐌰𐌽 𐌱𐍂𐌹𐌲𐌲𐌹𐌸.\n",
        "Old Norse (Runes): ᛖᚴ ᚷᛖᛏ ᛖᛏᛁ ᚧ ᚷᛚᛖᚱ ᛘᚾ ᚦᛖᛋᛋ ᚨᚧ ᚡᛖ ᚱᚧᚨ ᛋᚨᚱ\n",
        "Old Norse (Latin): Ek get etið gler án þess að verða sár.\n",
        "Norsk / Norwegian (Nynorsk): Eg kan eta glas utan å skada meg.\n",
        "Norsk / Norwegian (Bokmål): Jeg kan spise glass uten å skade meg.\n",
        "Føroyskt / Faroese: Eg kann eta glas, skaðaleysur.\n",
        "Íslenska / Icelandic: Ég get etið gler án þess að meiða mig.\n",
        "Svenska / Swedish: Jag kan äta glas utan att skada mig.\n",
        "Dansk / Danish: Jeg kan spise glas, det gør ikke ondt på mig.\n",
        "Sønderjysk: Æ ka æe glass uhen at det go mæ naue.\n",
        "Frysk / Frisian: Ik kin glês ite, it docht me net sear.\n",
        "Nederlands / Dutch: Ik kan glas eten, het doet mĳ geen kwaad.\n",
        "Kirchröadsj/Bôchesserplat: Iech ken glaas èèse, mer 't deet miech jing pieng.\n",
        "Afrikaans: Ek kan glas eet, maar dit doen my nie skade nie.\n",
        "Lëtzebuergescht / Luxemburgish: Ech kan Glas iessen, daat deet mir nët wei.\n",
        "Deutsch / German: Ich kann Glas essen, ohne mir zu schaden.\n",
        "Ruhrdeutsch: Ich kann Glas verkasematuckeln, ohne dattet mich wat jucken tut.\n",
        "Langenfelder Platt: Isch kann Jlaas kimmeln, uuhne datt mich datt weh dääd.\n",
        "Lausitzer Mundart (\"Lusatian\"): Ich koann Gloos assn und doas dudd merr ni wii.\n",
        "Odenwälderisch: Iech konn glaasch voschbachteln ohne dass es mir ebbs daun doun dud.\n",
        "Sächsisch / Saxon: 'sch kann Glos essn, ohne dass'sch mer wehtue.\n",
        "Pfälzisch: Isch konn Glass fresse ohne dasses mer ebbes ausmache dud.\n",
        "Schwäbisch / Swabian: I kå Glas frässa, ond des macht mr nix!\n",
        "Deutsch (Voralberg): I ka glas eassa, ohne dass mar weh tuat.\n",
        "Bayrisch / Bavarian: I koh Glos esa, und es duard ma ned wei.\n",
        "Allemannisch: I kaun Gloos essen, es tuat ma ned weh.\n",
        "Schwyzerdütsch (Zürich): Ich chan Glaas ässe, das schadt mir nöd.\n",
        "Schwyzerdütsch (Luzern): Ech cha Glâs ässe, das schadt mer ned.\n",
        "Plautdietsch: (NEEDED)\n",
        "Hungarian: Meg tudom enni az üveget, nem lesz tőle bajom.\n",
        "Suomi / Finnish: Voin syödä lasia, se ei vahingoita minua.\n",
        "Sami (Northern): Sáhtán borrat lása, dat ii leat bávččas.\n",
        "Erzian: Мон ярсан суликадо, ды зыян эйстэнзэ а ули.\n",
        "Northern Karelian: Mie voin syvvä lasie ta minla ei ole kipie.\n",
        "Southern Karelian: Minä voin syvvä st'oklua dai minule ei ole kibie.\n",
        "Vepsian: (NEEDED)\n",
        "Votian: (NEEDED)\n",
        "Livonian: (NEEDED)\n",
        "Estonian: Ma võin klaasi süüa, see ei tee mulle midagi.\n",
        "Latvian: Es varu ēst stiklu, tas man nekaitē.\n",
        "Lithuanian: Aš galiu valgyti stiklą ir jis manęs nežeidžia\n",
        "Old Prussian: (NEEDED)\n",
        "Sorbian (Wendish): (NEEDED)\n",
        "Czech: Mohu jíst sklo, neublíží mi.\n",
        "Slovak: Môžem jesť sklo. Nezraní ma.\n",
        "Polska / Polish: Mogę jeść szkło i mi nie szkodzi.\n",
        "Slovenian: Lahko jem steklo, ne da bi mi škodovalo.\n",
        "Bosnian, Croatian, Montenegrin and Serbian (Latin): Ja mogu jesti staklo, i to mi ne šteti.\n",
        "Bosnian, Montenegrin and Serbian (Cyrillic): Ја могу јести стакло, и то ми не штети.\n",
        "Macedonian: Можам да јадам стакло, а не ме штета.\n",
        "Russian: Я могу есть стекло, оно мне не вредит.\n",
        "Belarusian (Cyrillic): Я магу есці шкло, яно мне не шкодзіць.\n",
        "Belarusian (Lacinka): Ja mahu jeści škło, jano mne ne škodzić.\n",
        "Ukrainian: Я можу їсти скло, і воно мені не зашкодить.\n",
        "Bulgarian: Мога да ям стъкло, то не ми вреди.\n",
        "Georgian: მინას ვჭამ და არა მტკივა.\n",
        "Armenian: Կրնամ ապակի ուտել և ինծի անհանգիստ չըներ։\n",
        "Albanian: Unë mund të ha qelq dhe nuk më gjen gjë.\n",
        "Turkish: Cam yiyebilirim, bana zararı dokunmaz.\n",
        "Turkish (Ottoman): جام ييه بلورم بڭا ضررى طوقونمز\n",
        "Tatar: Алам да бар, пыяла, әмма бу ранит мине.\n",
        "Uzbek / O’zbekcha: (Roman): Men shisha yeyishim mumkin, ammo u menga zarar keltirmaydi.\n",
        "Uzbek / Ўзбекча (Cyrillic): Мен шиша ейишим мумкин, аммо у менга зарар келтирмайди.\n",
        "Bangla / Bengali: আমি কাঁচ খেতে পারি, তাতে আমার কোনো ক্ষতি হয় না।\n",
        "Marathi (masculine): मी काच खाऊ शकतो, मला ते दुखत नाही.\n",
        "Marathi (feminine):   मी काच खाऊ शकते, मला ते दुखत नाही.\n",
        "Kannada: ನನಗೆ ಹಾನಿ ಆಗದೆ, ನಾನು ಗಜನ್ನು ತಿನಬಹುದು\n",
        "Hindi (masculine): मैं काँच खा सकता हूँ और मुझे उससे कोई चोट नहीं पहुंचती.\n",
        "Hindi (feminine):   मैं काँच खा सकती हूँ और मुझे उससे कोई चोट नहीं पहुंचती.\n",
        "Malayalam: എനിക്ക് ഗ്ലാസ് തിന്നാം. അതെന്നെ വേദനിപ്പിക്കില്ല.\n",
        "Tamil: நான் கண்ணாடி சாப்பிடுவேன், அதனால் எனக்கு ஒரு கேடும் வராது.\n",
        "Telugu: నేను గాజు తినగలను మరియు అలా చేసినా నాకు ఏమి ఇబ్బంది లేదు\n",
        "Sinhalese: මට වීදුරු කෑමට හැකියි. එයින් මට කිසි හානියක් සිදු නොවේ.\n",
        "Urdu(3): میں کانچ کھا سکتا ہوں اور مجھے تکلیف نہیں ہوتی ۔\n",
        "Pashto(3): زه شيشه خوړلې شم، هغه ما نه خوږوي\n",
        "Farsi / Persian(3): .من می توانم بدونِ احساس درد شيشه بخورم\n",
        "Arabic(3): أنا قادر على أكل الزجاج و هذا لا يؤلمني.\n",
        "Aramaic: (NEEDED)\n",
        "Maltese: Nista' niekol il-ħġieġ u ma jagħmilli xejn.\n",
        "Hebrew(3): אני יכול לאכול זכוכית וזה לא מזיק לי.\n",
        "Yiddish(3): איך קען עסן גלאָז און עס טוט מיר נישט װײ.\n",
        "Judeo-Arabic: (NEEDED)\n",
        "Ladino: (NEEDED)\n",
        "Gǝʼǝz: (NEEDED)\n",
        "Amharic: (NEEDED)\n",
        "Twi: Metumi awe tumpan, ɜnyɜ me hwee.\n",
        "Hausa (Latin): Inā iya taunar gilāshi kuma in gamā lāfiyā.\n",
        "Hausa (Ajami) (2): إِنا إِىَ تَونَر غِلَاشِ كُمَ إِن غَمَا لَافِىَا\n",
        "Yoruba(4): Mo lè je̩ dígí, kò ní pa mí lára.\n",
        "Lingala: Nakokí kolíya biténi bya milungi, ekosála ngáí mabé tɛ́.\n",
        "(Ki)Swahili: Naweza kula bilauri na sikunyui.\n",
        "Malay: Saya boleh makan kaca dan ia tidak mencederakan saya.\n",
        "Tagalog: Kaya kong kumain nang bubog at hindi ako masaktan.\n",
        "Chamorro: Siña yo' chumocho krestat, ti ha na'lalamen yo'.\n",
        "Fijian: Au rawa ni kana iloilo, ia au sega ni vakacacani kina.\n",
        "Javanese: Aku isa mangan beling tanpa lara.\n",
        "Burmese (Unicode 4.0): က္ယ္ဝန္‌တော္‌၊က္ယ္ဝန္‌မ မ္ယက္‌စားနုိင္‌သည္‌။ ၎က္ရောင္‌့ ထိခုိက္‌မ္ဟု မရ္ဟိပာ။ (9)\n",
        "Burmese (Unicode 5.0): ကျွန်တော် ကျွန်မ မှန်စားနိုင်တယ်။ ၎င်းကြောင့် ထိခိုက်မှုမရှိပါ။ (9)\n",
        "Vietnamese (quốc ngữ): Tôi có thể ăn thủy tinh mà không hại gì.\n",
        "Vietnamese (nôm) (4): 些 𣎏 世 咹 水 晶 𦓡 空 𣎏 害 咦\n",
        "Khmer: ខ្ញុំអាចញុំកញ្ចក់បាន ដោយគ្មានបញ្ហារ\n",
        "Lao: ຂອ້ຍກິນແກ້ວໄດ້ໂດຍທີ່ມັນບໍ່ໄດ້ເຮັດໃຫ້ຂອ້ຍເຈັບ.\n",
        "Thai: ฉันกินกระจกได้ แต่มันไม่ทำให้ฉันเจ็บ\n",
        "Mongolian (Cyrillic): Би шил идэй чадна, надад хортой биш\n",
        "Mongolian (Classic) (5): ᠪᠢ ᠰᠢᠯᠢ ᠢᠳᠡᠶᠦ ᠴᠢᠳᠠᠨᠠ ᠂ ᠨᠠᠳᠤᠷ ᠬᠣᠤᠷᠠᠳᠠᠢ ᠪᠢᠰᠢ\n",
        "Dzongkha: (NEEDED)\n",
        "Nepali: ﻿म काँच खान सक्छू र मलाई केहि नी हुन्‍न् ।\n",
        "Tibetan: ཤེལ་སྒོ་ཟ་ནས་ང་ན་གི་མ་རེད།\n",
        "Chinese: 我能吞下玻璃而不伤身体。\n",
        "Chinese (Traditional): 我能吞下玻璃而不傷身體。\n",
        "Taiwanese(6): Góa ē-tàng chia̍h po-lê, mā bē tio̍h-siong.\n",
        "Japanese: 私はガラスを食べられます。それは私を傷つけません。\n",
        "Korean: 나는 유리를 먹을 수 있어요. 그래도 아프지 않아요\n",
        "Bislama: Mi save kakae glas, hemi no save katem mi.\n",
        "Hawaiian: Hiki iaʻu ke ʻai i ke aniani; ʻaʻole nō lā au e ʻeha.\n",
        "Marquesan: E koʻana e kai i te karahi, mea ʻā, ʻaʻe hauhau.\n",
        "Inuktitut (10): ᐊᓕᒍᖅ ᓂᕆᔭᕌᖓᒃᑯ ᓱᕋᙱᑦᑐᓐᓇᖅᑐᖓ\n",
        "Chinook Jargon: Naika məkmək kakshət labutay, pi weyk ukuk munk-sik nay.\n",
        "Navajo: Tsésǫʼ yishą́ągo bííníshghah dóó doo shił neezgai da.\n",
        "Cherokee (and Cree, Chickasaw, Cree, Micmac, Ojibwa, Lakota, Náhuatl, Quechua, Aymara, and other American languages): (NEEDED)\n",
        "Garifuna: (NEEDED)\n",
        "Gullah: (NEEDED)\n",
        "Lojban: mi kakne le nu citka le blaci .iku'i le se go'i na xrani mi\n",
        "Nórdicg: Ljœr ye caudran créneþ ý jor cẃran.\n",
        "''']\n",
        "\n",
        "for q in QQQ:\n",
        "    tokens = tokenizer_rust.encode(q)\n",
        "    if q != tokenizer_rust.decode(tokens):\n",
        "        print('ERROR', q)\n",
        "\n",
        "print('All OK\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAJ500hkLzC-",
        "outputId": "070c90a7-5c8e-4dd7-c216-54c1d41f458f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All OK\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Su_Tc9RGL8PJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsMrYg0N1MGoisVWOszDOj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}